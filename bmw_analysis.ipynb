{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "great-sperm",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Predict resale prices of BMW cars. This could for instance be used by someone who wants to sell their car, to get an idea about how much it is worth, similar to how Kelley Blue Book works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-joining",
   "metadata": {},
   "source": [
    "# Thinking about the problem\n",
    "From the readme of the dataset available here <https://github.com/datacamp/careerhub-data/tree/master/BMW%20Used%20Car%20Sales>, one can see that the dataset contains information about price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size. Upon inspection of the dataset (see below), it turned out to additionally contain the car model and year (I'm assuming this means production year). First I want to describe my initial expectations for the relationships between these quantities, and formulate different levels of complexity for including the data.\n",
    "\n",
    "The five quantities model, year, transmission, fuel type, and engine size collectively describe the car configuration at the time of initial purchase. The quantity mileage describes how much the car has been used, and therefore worn since that point. The quantities miles per gallon and road tax should be given based on the new car configuration quantities.\n",
    "\n",
    "I suspect that the price will strongly depend on the mileage and age of the car, and a first simple model could therefore just consider these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires the file draw_diagrams.py to be in the same directory as this notebook\n",
    "import draw_diagrams\n",
    "draw_diagrams.data_model1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbd40e",
   "metadata": {},
   "source": [
    "An improvement on this would be to include the new car configuration variables. From these in addition to price, mpg and road tax could be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77763c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_diagrams.data_model2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cec5fa",
   "metadata": {},
   "source": [
    "Finally the last two variables, mpg and road tax, can be included. These could affect the resale price of the car, since they would probably influence how much a buyer is willing to pay, but I suspect this connection will be less strong than the connection between the other variables and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753404c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_diagrams.data_model3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59be1b",
   "metadata": {},
   "source": [
    "Before any of this though, first I want to take a closer at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630ab74",
   "metadata": {},
   "source": [
    "\n",
    "# Loading and inspecting data\n",
    "First I load and inspect the data. I downloaded the data from [here](https://raw.githubusercontent.com/datacamp/careerhub-data/master/BMW%20Used%20Car%20Sales/bmw.csv) and saved it in the `datasets/bmw.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw = pd.read_csv(\"datasets/bmw.csv\")\n",
    "bmw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_values(bmw):\n",
    "    for col in [\"model\", \"fuelType\", \"transmission\"]:\n",
    "        print(col)\n",
    "        print(list(bmw[col].unique()))\n",
    "\n",
    "\n",
    "print_category_values(bmw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in bmw:\n",
    "    print(col, len(bmw[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-laundry",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "<a id = \"data-exploration\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de60b4",
   "metadata": {},
   "source": [
    "Let's look at how the price depends on all the continous variables using a pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    bmw,  # hue='transmission',\n",
    "    x_vars=[\"price\", \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"],\n",
    "    y_vars=[\"price\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74cdb8",
   "metadata": {},
   "source": [
    "There appears to be a definite relationship between price and both mileage and year. The relationship looks like at might be expopnential, so let's we look at the logarithm of the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c108e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bmw_log = bmw.copy()\n",
    "bmw_log['log price'] = np.log10(bmw_log['price'])\n",
    "bmw_log = bmw_log.drop('price', axis='columns')\n",
    "\n",
    "sns.pairplot(bmw_log, #hue='transmission', \n",
    "             x_vars=['log price', 'year', 'mileage',  'tax', 'mpg', 'engineSize'],\n",
    "             y_vars=['log price']) #, hue='transmission')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571565f8",
   "metadata": {},
   "source": [
    "These plots reveal that there appears to be a linear relationship between the logarithm of the price, and both year and mileage. There is no obvious relationship between the price and the remaining variables, whether we consider logarithm or not. Going forward in the analysis, we will be using the logarithm of the price as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52f512",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "## Categorical variables\n",
    "Let us take a closer look at the categorical columns. First we print the number of values in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"model\", \"fuelType\", \"transmission\"]\n",
    "\n",
    "\n",
    "def print_categorical_counts(df, columns):\n",
    "    for col in columns:\n",
    "        display(df.groupby(col)[col].count())\n",
    "\n",
    "\n",
    "print_categorical_counts(bmw_log, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f6b14",
   "metadata": {},
   "source": [
    "There are a number of categories with very few records. For instance, the `fuelType` `Electric` has only three. With such a small amount of observations for this category, and no obvious relationship with other entries in this category as one naturally has for numeric columns, I wouldn't expect it to be possible to make reliable predictions for the selling price for this category. I therefore choose to drop any category with less than 10 records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_almost_empty_categories(df, col, nmin=10):\n",
    "    df = df.copy()  # To avoid modyfiyng the input dataframe\n",
    "    category_count = df.groupby(col)[col].count()\n",
    "    for category_name, count in category_count.iteritems():\n",
    "        if count < nmin:\n",
    "            df = df[df[col] != category_name]\n",
    "    return df\n",
    "\n",
    "\n",
    "bmw_cat = bmw_log.copy()\n",
    "for col in categorical_columns:\n",
    "    bmw_cat = drop_almost_empty_categories(bmw_cat, col)\n",
    "bmw_cat[categorical_columns] = bmw_cat[categorical_columns].astype('category')\n",
    "# print_categorical_counts(bmw_cat, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e005dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car_config_cols = ['model', 'transmission', 'fuelType', 'engineSize']\n",
    "new_car_cols = new_car_config_cols + ['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    bmw.sort_values(\"engineSize\"),  # hue='transmission',\n",
    "    x_vars=new_car_cols,\n",
    "    y_vars=new_car_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb752bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bmw_cat[bmw_cat.engineSize==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9caed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    new_car_grouped = bmw.groupby(new_car_cols)[[\"tax\", \"mpg\", \"price\"]]\n",
    "    display(new_car_grouped.nunique())\n",
    "    # display(bmw.groupby(new_car_config_cols)['tax'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95172c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "choices = (\n",
    "    (bmw.model == \" 1 Series\")\n",
    "    & (bmw.transmission == \"Automatic\")\n",
    "    & (bmw.fuelType == \"Diesel\")\n",
    "    & (bmw.engineSize == 2.0)\n",
    "    & (bmw.year == 2016)\n",
    ")\n",
    "bmw[choices][[\"mileage\", \"tax\", \"mpg\", \"price\"]].sort_values(\"mileage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadddcd",
   "metadata": {},
   "source": [
    "## A bit more data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46d663",
   "metadata": {},
   "source": [
    "From the plots we can see that `mpg` has a group of values near 400, far from the nearest values which are less than 200. Let's see how many different values  are present there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023dfd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw_cat[bmw_cat[\"mpg\"]>400][\"mpg\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac1c16",
   "metadata": {},
   "source": [
    "All the values of `mpg` in the group near 400 have the same value. This looks very suspicious. I suspect this is data is wrong, and since it could seriously skew a model since it has such high values, I should eliminate these values (either impute with e.g. average, or drop the records all together).\n",
    "\n",
    "Let's also check the remaining two continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f454d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(sorted(bmw_dropped[\"engineSize\"].unique()))\n",
    "display(bmw_cat.groupby(\"engineSize\")[\"engineSize\"].count())\n",
    "bmw_cat.groupby(\"tax\")[\"tax\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d66066",
   "metadata": {},
   "source": [
    "They both contain zeros, which seems weird for both tax and engine size. The skewing effect is probably less then for the `mpg` outliers, since zero is closer to other values of tax and engine size, but I should still either impute or drop these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_dropped = (bmw_cat.mpg > 400) | (bmw_cat.engineSize == 0) | (bmw_cat.tax == 0)\n",
    "bmw_cleaned = bmw_cat[~to_be_dropped]\n",
    "# bmw_cleaned = bmw_log\n",
    "# bmw_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79f9e0",
   "metadata": {},
   "source": [
    "# Regression models\n",
    "Here I train models on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe21ae",
   "metadata": {},
   "source": [
    "## Linear regression models\n",
    "For the first model, I only want to consider the dependency of price on build year and mileage. From the plots in the [data exploration](#data-exploration) section we see that the logarithm of the price appears to depend linearly on year and mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def print_linear_coeffs(features, linreg, std=None):\n",
    "    coeffs = pd.DataFrame(\n",
    "        {\n",
    "            \"observable\": features,\n",
    "            \"coef\": linreg.coef_,\n",
    "            \"10^coef\": np.power(10, linreg.coef_),\n",
    "        }\n",
    "    )\n",
    "    coeffs = coeffs.set_index(\"observable\")\n",
    "    if std is not None:\n",
    "        coeffs[\"std\"] = std\n",
    "        coeffs[\"coef*std\"] = coeffs[\"std\"] * coeffs[\"coef\"]\n",
    "        coeffs = coeffs.sort_values(\"coef*std\", key=np.abs, ascending=False)\n",
    "    display(coeffs)\n",
    "\n",
    "\n",
    "def every_column_name_but(df, dependent):\n",
    "    features = [col for col in df.columns if col != dependent]\n",
    "    return features\n",
    "\n",
    "\n",
    "def split_dependent(df, features=\"all\", dependent=\"log price\"):\n",
    "    if features == \"all\":\n",
    "        features = every_column_name_but(df, dependent)\n",
    "    else:\n",
    "        features = [col for col in features if col in df.columns and col != dependent]\n",
    "    return df[features], df[dependent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmw_train, bmw_val = train_test_split(bmw_cleaned, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_ohe(drop=\"first\"):\n",
    "    \"\"\"Make a one hot encoder that only acts on categorical columns\"\"\"\n",
    "    cat_transformer_tuple = (\n",
    "        OneHotEncoder(drop=drop),\n",
    "        make_column_selector(dtype_include=\"category\"),\n",
    "    )\n",
    "    ohe = make_column_transformer(cat_transformer_tuple, remainder=\"passthrough\")\n",
    "    return ohe\n",
    "\n",
    "ohe = make_cat_ohe()\n",
    "\n",
    "linreg = Pipeline(((\"one_hot\", ohe), (\"regressor\", LinearRegression())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04198be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_dependent(bmw_train[[\"log price\", \"mileage\", \"year\"]], dependent=\"log price\")\n",
    "cross_validate(linreg, X, y, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e836e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_mean_and_std(scores):\n",
    "    \"\"\"Finds mean and standard deviations of scores from `cross_validate`,\n",
    "    and puts them in a dataframe.\"\"\"\n",
    "    scores = pd.DataFrame(scores)[[\"test_score\", \"train_score\"]]\n",
    "    mean = scores.mean().add_prefix(\"mean_\")\n",
    "    std = scores.std().add_prefix(\"std_\")\n",
    "    mean_std = pd.concat((mean, std))\n",
    "    return mean_std\n",
    "\n",
    "feature_cols = [\"mileage\", \"model\", \"year\", \"engineSize\", \"transmission\", \"fuelType\", \"mpg\", \"tax\"]\n",
    "#feature_cols = [\"mileage\", \"model\", \"year\", \"engineSize\", \"fuelType\", \"transmission\", \"mpg\", \"tax\"]\n",
    "linreg = Pipeline(((\"one_hot\", make_cat_ohe()), (\"regressor\", LinearRegression())))\n",
    "all_scores = {}\n",
    "for i in range(1, len(feature_cols) + 1):\n",
    "    cols = [\"log price\"] + feature_cols[:i]\n",
    "    X, y = split_dependent(bmw_train[cols], dependent=\"log price\")\n",
    "    scores = cross_validate(linreg, X, y, return_train_score=True)\n",
    "    all_scores[cols[-1]] = scores_mean_and_std(scores)\n",
    "all_scores = pd.DataFrame(all_scores).T\n",
    "all_scores.index.name = \"Last added feature\"\n",
    "display(all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403ee1c",
   "metadata": {},
   "source": [
    "Here I cumulatively added features one by one, and look at the five-fold cross validation score from fitting a linear model. I see that only considering the `mileage` gives a low R^2 score of 0.432. Adding the car `model` improves it considerably, as does adding `year`. Further adding the remaining new car configuration features further improves the R^2 score. Adding the `mpg` and `tax` does not change the R^2 score much though. We therefore continue the analysis including only the `mileage` and the new car configuration features, but excluding `mpg` and `tax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7162c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"log price\",\n",
    "    \"mileage\",\n",
    "    \"model\",\n",
    "    \"year\",\n",
    "    \"engineSize\",\n",
    "    \"transmission\",\n",
    "    \"fuelType\",\n",
    "]\n",
    "bmw_reduced = bmw_cleaned[cols]\n",
    "bmw_reduced_train = bmw_train[cols]\n",
    "bmw_reduced_val = bmw_val[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2c728",
   "metadata": {},
   "source": [
    "## Lasso and Ridge\n",
    "The training and test scores of the linear models fitted are not very different, which indicates that the model is not overfitting much. This is also expectable for linear models, since these models tend to have large bias, but low variance.\n",
    "\n",
    "To make sure that the data has not been overfitted, I performed lasso and ridge regressions, for a series of values of their `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b44952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Pipeline(((\"one_hot\", make_cat_ohe()), (\"regressor\", Lasso())))\n",
    "\n",
    "param_grid = {\"regressor__alpha\": [0.0005, 0.001, 0.01, 0.1]}\n",
    "clf = GridSearchCV(estimator=lasso, param_grid=param_grid, return_train_score=True)\n",
    "X, y = split_dependent(bmw_reduced, dependent=\"log price\")\n",
    "clf.fit(X, y)\n",
    "display(\n",
    "    pd.DataFrame(clf.cv_results_)[\n",
    "        [\n",
    "            \"param_regressor__alpha\",\n",
    "            \"mean_test_score\",\n",
    "            \"mean_train_score\",\n",
    "            \"std_test_score\",\n",
    "            \"std_train_score\",\n",
    "        ]\n",
    "    ].set_index(\"param_regressor__alpha\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5803ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge = Pipeline(((\"one_hot\", make_cat_ohe()), (\"regressor\", Ridge(tol=1e-9))))\n",
    "\n",
    "param_grid = {\"regressor__alpha\": [0] + list(10**i for i in range(5))}\n",
    "clf = GridSearchCV(estimator=ridge, param_grid=param_grid, return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "display(\n",
    "    pd.DataFrame(clf.cv_results_)[\n",
    "        [\n",
    "            \"param_regressor__alpha\",\n",
    "            \"mean_test_score\",\n",
    "            \"mean_train_score\",\n",
    "            \"std_test_score\",\n",
    "            \"std_train_score\",\n",
    "        ]\n",
    "    ].set_index(\"param_regressor__alpha\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338602c4",
   "metadata": {},
   "source": [
    "We see that neither lasso or ridge regression are able to decrease the difference between test and the training score, which I also expected from the small initial difference between the two. I therefore continue the analysis without any of the regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbd1a",
   "metadata": {},
   "source": [
    "## Validation check\n",
    "Finally, I check the model using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_dependent(bmw_reduced_train, dependent=\"log price\")\n",
    "linreg.fit(X, y)\n",
    "X_val, y_val = split_dependent(bmw_reduced_val, dependent=\"log price\")\n",
    "y_predict_val = linreg.predict(X_val)\n",
    "r2_score(y_val, y_predict_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b07ea1",
   "metadata": {},
   "source": [
    "The obtained validation R^2 score is close to the R^2 from the cross-validation, which once again indicates that the model is not overfitting. I go ahead with this model, refitting it to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c04f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_dependent(bmw_reduced, dependent=\"log price\")\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc2dce",
   "metadata": {},
   "source": [
    "## Parameter interpretation\n",
    "A nice property of the linear regression model is that it's coefficients has straightforward interpretations. Below I print these coefficients, together with the standard deviation in the corresponding variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12c146",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_cols = bmw_reduced.columns[bmw_reduced.dtypes == \"category\"]\n",
    "\n",
    "\n",
    "def rename_ohe_features(features, cat_cols):\n",
    "    for i, cat_col in enumerate(cat_cols):\n",
    "        features = [\n",
    "            feature.replace(f\"onehotencoder__x{i}\", cat_col) for feature in features\n",
    "        ]\n",
    "    return features\n",
    "\n",
    "\n",
    "features = linreg.named_steps[\"one_hot\"].get_feature_names()\n",
    "features = rename_ohe_features(features, cat_cols)\n",
    "\n",
    "std = pd.get_dummies(bmw_reduced).std()\n",
    "print_linear_coeffs(features, linreg.named_steps[\"regressor\"], std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfad0c",
   "metadata": {},
   "source": [
    "I sorted the coefficients by the product of the coefficient and the standard deviation. This product gives a measure of how important the feature is in the model. Since we fit to the logarithm of the price, I also show $10^{\\text{coeff}}$. This can be interpreted as multiplicative factor, modifying the price depending on the value of the feature. For instance, `year` has a value of $10^\\text{coef}=1.11$, which means that a car would be 1.107 times more expensive than a corresponding one year older car.\n",
    "For the categorical variables, such as model, $10^\\text{coef}$ describes the relative price between the different categories. For instance, `model_X5` has $10^\\text{coef}=1.71$, meaning that an X5 car is 1.71 times more expensive than the first model, which was dropped by the `OneHotEncoder`. How many times more expensive one model is compared to another can be found by dividing their values of $10^\\text{coef}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a7a3c",
   "metadata": {},
   "source": [
    "## Prediction intervals\n",
    "Only a single number of returned by the linear model above when it is given the data for a car. But we would generally expect that the real prices are distributed with some variance around this value. It would be nice though to have some kind of idea as to how accurate that number is. One way to indicate this is with a prediction interval.\n",
    "A prediction interval is an interval of prices, in which we with some percentage (say 90%) of confidence can say that the price of the car would be within. Note that this is different from the confidence interval, which specifies how confidently we can say that we have predicted the mean distribution, but it says nothing about the variance.\n",
    "\n",
    "Since prediction intervals does not appear to be included in scikit-learn, I will do my own simple implementation here.\n",
    "From <https://online.stat.psu.edu/stat501/lesson/3/3.3> we have the following expression for the prediction interval\n",
    "\n",
    "$$\\hat{y}_h \\pm t_{(1-\\alpha/2, n-2)}\\sqrt{MSE \\cdot \\left(1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum_i(x_i-\\bar{x})^2}\\right)}$$\n",
    "\n",
    "where $\\hat{y}_k$ is the fitted value, $t_{(1-\\alpha/2, n-2)}$ is the pdf of a t-distribution and $n$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5991db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "n = len(bmw_reduced)\n",
    "alpha=0.90\n",
    "t.ppf((1+alpha)/2, n-2)\n",
    "MSE = mean_squared_error(y, linreg.predict(X))\n",
    "X_ohe = linreg.named_steps[\"one_hot\"].transform(X)\n",
    "X_ohe - np.mean(X_ohe, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cebfb5",
   "metadata": {},
   "source": [
    "Following <https://saattrupdan.github.io/2020-02-26-parametric-prediction/> I implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "def calc_prediction_delta(y, y_pred, alpha=0.90, print_ratio_captured=False):\n",
    "    n = len(y)\n",
    "    resid = y-y_pred\n",
    "    mean_resid = np.mean(y-y_pred)\n",
    "    sN2 = 1/(n-1)*sum((resid-mean_resid)**2)\n",
    "    dy = t.ppf((1+alpha)/2, n-1)*np.sqrt(sN2)*(1+1/n)\n",
    "    if print_ratio_captured:\n",
    "        print(\"Ratio in prediction interval\", np.mean(np.abs(resid + mean_resid) < dy))\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12):\n",
    "    X_, y_ = split_dependent(bmw_reduced[:i*1000])\n",
    "    dy = calc_prediction_delta(y_, linreg.predict(X_), print_ratio_captured=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39032d",
   "metadata": {},
   "source": [
    "Calculating this over the BMW dataset gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = calc_prediction_delta(y, linreg.predict(X), alpha=0.90, print_ratio_captured=True)\n",
    "print(\"dy\", dy)\n",
    "print(\"10^dy\", 10**dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6931230",
   "metadata": {},
   "source": [
    "Here we see that indeed around 90% of the prices were within the 90% prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_with_pred_interval(X, linreg, dy):\n",
    "    y_predict = linreg.predict(X[:10])\n",
    "    y_pred_w_interval = pd.DataFrame(\n",
    "        {\"y\": y_predict, \"y-dy\": y_predict - dy, \"y+dy\": y_predict + dy}\n",
    "    )\n",
    "    price = np.power(10, y_pred_w_interval).rename(\n",
    "        {\"y\": \"price\", \"y-dy\": \"upper\", \"y+dy\": \"lower\"}, axis=\"columns\"\n",
    "    )\n",
    "    print(price)\n",
    "\n",
    "\n",
    "price_with_pred_interval(X[:10], linreg, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1814715",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"mileage\", \"model\", \"year\", \"engineSize\", \"transmission\", \"fuelType\", \"mpg\", \"tax\"]\n",
    "#feature_cols = [\"mileage\", \"model\", \"year\", \"engineSize\", \"fuelType\", \"transmission\", \"mpg\", \"tax\"]\n",
    "linreg_play = Pipeline(((\"one_hot\", make_cat_ohe()), (\"regressor\", LinearRegression())))\n",
    "all_dys = {}\n",
    "for i in range(1, len(feature_cols) + 1):\n",
    "    cols = [\"log price\"] + feature_cols[:i]\n",
    "    X, y = split_dependent(bmw_train[cols], dependent=\"log price\")\n",
    "    linreg_play.fit(X, y)\n",
    "    dy = calc_prediction_delta(y, linreg_play.predict(X), alpha=0.90, print_ratio_captured=True)\n",
    "    all_dys[cols[-1]] = dy\n",
    "all_dys = pd.Series(all_dys)\n",
    "all_dys = pd.DataFrame({\"dy\": all_dys, \"10^dy\": np.power(10, all_dys)})\n",
    "\n",
    "all_dys.index.name = \"Last added feature\"\n",
    "display(all_dys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508b07d",
   "metadata": {},
   "source": [
    "# Making predictions with partial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "def powerset(iterable, start=0):\n",
    "    \"\"\"\"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n",
    "    This function comes from the python documentation at https://docs.python.org/3/library/itertools.html\"\"\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(start, len(s)+1))\n",
    "\n",
    "def train_set_of_models(features, bmw, dependent=\"log price\"):\n",
    "    models = {}\n",
    "    for feature_set in powerset(features, start=1):\n",
    "        linreg_local = Pipeline(((\"one_hot\", make_cat_ohe()), (\"regressor\", LinearRegression())))\n",
    "        X = bmw[list(feature_set)]\n",
    "        y = bmw[dependent]\n",
    "        linreg_local.fit(X, y)\n",
    "        dy = calc_prediction_delta(y, linreg_local.predict(X), alpha=0.90)\n",
    "        models[feature_set] = {\"model\": linreg_local, \"dy_90\": dy}\n",
    "    return models\n",
    "#        print(\"feature_set\", feature_set)\n",
    "\n",
    "\n",
    "included_features = [\"mileage\", \"model\", \"year\", \"engineSize\", \"transmission\"]\n",
    "#included_features = included_features[:2]\n",
    "models = train_set_of_models(included_features, bmw_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76563b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_element(keys):\n",
    "    return keys[np.argmax(list(map(len, keys)))]\n",
    "\n",
    "\n",
    "def scalar_to_list(x):\n",
    "    if np.isscalar(x):\n",
    "        return [x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def eval_model(models, **kwargs):\n",
    "    features = find_longest_element(list(models.keys()))\n",
    "    #print(features)\n",
    "    for key in kwargs:\n",
    "        if key not in features:\n",
    "            raise ValueError(f\"{key} not found in {features}\")\n",
    "    chosen_features = tuple(feature for feature in features if feature in kwargs)\n",
    "    model_holder = models[chosen_features]\n",
    "    model = model_holder[\"model\"]\n",
    "    dy = model_holder[\"dy_90\"]\n",
    "\n",
    "    values = (scalar_to_list(kwargs[feature]) for feature in chosen_features)\n",
    "    X = pd.DataFrame(\n",
    "        dict(\n",
    "            zip(\n",
    "                chosen_features,\n",
    "                values,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    price = np.power(10, model.predict(X))\n",
    "    lower_90 = np.power(10, model.predict(X)-dy)\n",
    "    upper_90 = np.power(10, model.predict(X)+dy)\n",
    "    price_w_interval = pd.DataFrame(\n",
    "        {\"price\": price, \"90% lower bound\": lower_90, \"90% upper bound\": upper_90}\n",
    "    )\n",
    "\n",
    "    return price_w_interval\n",
    "\n",
    "\n",
    "eval_model(models, mileage=[20, 400, 500], model=3*[\" 2 Series\"])\n",
    "#eval_model(models, mileage=[20], model=\" 2 Series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c19823",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_values(bmw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002e6f0",
   "metadata": {},
   "source": [
    "# Dumping models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ada2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "model_dump_file = \"bmw_linreg_model.pckl\"\n",
    "with open(model_dump_file, \"wb\") as file_:\n",
    "    dump(models, file_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b77753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_feature_ranges(df):\n",
    "    features_ranges = {}\n",
    "    for col in df.select_dtypes(include=np.number):\n",
    "        series = df[col]\n",
    "        summary = {\n",
    "            \"type\": \"numeric\",\n",
    "            \"range\": (float(series.min()), float(series.max())),\n",
    "        }\n",
    "        features_ranges[col] = summary\n",
    "    for col in df.select_dtypes(include=\"category\"):\n",
    "        series = df[col]\n",
    "        summary = {\"type\": \"category\", \"values\": list(series.cat.categories)}\n",
    "        features_ranges[col] = summary\n",
    "    return features_ranges\n",
    "\n",
    "\n",
    "def dump_feature_ranges_to_json_file(df, filename=\"feature_ranges.json\"):\n",
    "    feature_ranges = extract_feature_ranges(df)\n",
    "    print(feature_ranges)\n",
    "    with open(filename, \"w\") as fil:\n",
    "        json.dump(feature_ranges, fil)\n",
    "\n",
    "\n",
    "dump_feature_ranges_to_json_file(X[included_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12902222",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, model_dict in models.items():\n",
    "    print(round(100*model_dict[\"dy_90\"]), key, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976527a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
